<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://nikhilshagri.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nikhilshagri.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-11T20:36:52+00:00</updated><id>https://nikhilshagri.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">List-decodability of Random Linear Codes</title><link href="https://nikhilshagri.github.io/blog/2024/ghk-list-decodability/" rel="alternate" type="text/html" title="List-decodability of Random Linear Codes"/><published>2024-07-12T00:00:00+00:00</published><updated>2024-07-12T00:00:00+00:00</updated><id>https://nikhilshagri.github.io/blog/2024/ghk-list-decodability</id><content type="html" xml:base="https://nikhilshagri.github.io/blog/2024/ghk-list-decodability/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>List-decodability (at least the combinatorial aspect of it) is concerned with producing codes where only a few codewords fall within any given Hamming ball of small radius. A code \(\calC \subseteq \mathbb{F}_2^n\) is said to be list-decodable with list size \(L\) and radius \(\rho\) if at most \(L\) codewords lie within every Hamming Ball in \(\mathbb{F}_2^n\) of radius \(\rho \cdot n\). A natural question to ask about list-decodable codes is: for a fixed (normalized) radius \(\rho\) and list size \(L\), what is the maximum rate that a \((\rho, L)\) list-decodable code can have? A simple computation proves that it is at most \(1-h(\rho) + o(1)\), where \(h: [0, 1] \rightarrow [0, 1]\) is the binary entropy function. The quantity \(1-h(\rho)\) is also known as the <em>list-decoding capacity</em>.</p> <p>On the positive side, we know of the existence of codes reaching the list-decoding capacity, by the probabilistic method. Specifically, for any small \(\epsilon &gt; 0\), a random code having rate \[ 1-h(\rho)-\epsilon \] is list-decodable with list size \(O(1/\epsilon)\), with high probability. A proof sketch for this result is as follows: pick a random code of rate \(r\) by independently choosing \(2^{rn}\) vectors uniformly from \(\mathbb{F}_2^n\). For some fixed set of \(L+1\) vectors, the probability that all vectors from this fixed set are simultaneously contained in the random code is at most \[ \left(\frac{2^{rn}}{2^n}\right)^{L+1}. \] Moreover, for a fixed Hamming ball centered at some vector \(y \in \mathbb{F}_2^n\), there are at most \((2^{h(\rho)n})^{L+1}\) “bad sets” that the random code must avoid containing. Lastly, there are \(2^n\) Hamming balls in total, centered around each of the \(2^n\) vectors in \(\mathbb{F}_2^n\). Therefore, we require: \[ 2^n \cdot (2^{h(\rho)n})^{L+1} \cdot \left(\frac{2^{rn}}{2^n}\right)^{L+1} &lt; 1. \] Solving for \(r\), we see that \(r\) can only be at most \(1-h(\rho)-1/(L+1)\). Upon denoting \(\epsilon = 1/(L+1)\), we get that a random code of rate at most \[ 1-h(\rho)-\epsilon \] is list-decodable with list size \(L = O(1/\epsilon)\), which is what we wanted to prove.</p> <p>For linear codes, a probabilistic proof similar to the one used for the result above gives a much weaker bound. Specifically, it guarantees that a random linear code having rate \(1-h(\rho)-\epsilon\) is list-decodable with list size \(exp(O(1/\epsilon))\). Note that the list size in this case is exponentially larger than the one guaranteed for random codes above. The point where the proof differs from the one above is where the probability of a random linear code excluding a fixed set of \(L+1\) many vectors needs to be calculated. Because every codeword is independent of one another in a random code, we can simply multiply the probabilities of a random codeword equaling some fixed vector. However, this is not possible with a random linear code, because the codewords are not linearly independent. (In fact, they are not even 3-wise independent, because if \(X_1\) and \(X_2\) are two codewords from a linear code \(\mathcal{C}\), then we know that \(X_1 + X_2\) also belongs to \(\mathcal{C}\).) But we can still use a weaker property: every set of \(L+1\) vectors has a linearly independent subset of size at least \(\log(L+1)\), and a necessary condition for all \(L+1\) points to lie within a Hamming ball is for the linearly independent subset to lie within the aforementioned Hamming ball. However, as we noted above, this workaround significantly worsens the bound on the list size, bumping it up from \(O(1/\epsilon)\) to \(exp(O(1/\epsilon))\).</p> <p>Following this, the next development was by Guruswami, Håstad, Sudan and Zuckerman<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>, where they proved the <em>existence</em> of linear binary \((\rho, 1/\epsilon)\)-list-decodable codes having rate \(1-h_2 (\rho)-\epsilon\). Even though they got very good upper bounds on the list size, the result fell short of claiming the existence of such codes <em>with high probability</em>. The proof followed a potential function argument; they showed that if we build the linear code one basis vector at a time, then a certain, carefully crafted potential function does not increase by much each time a basis vector is added, and so in the end, the quantity is still small. The smallness of this quantity in the end implies an upper bound of \(1/\epsilon + o(1)\) on the list size. (In 2018, Li and Wootters<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> were able to tweak this argument to show that the result does hold with high probability.)</p> <p>In 2010, a paper by Guruswami, Håstad and Kopparty<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> used a different technique to show that such random linear codes exist with high probability. This is the proof technique I want to talk about in this blog post, because it makes use of a clever little structural theorem concerning the intersection of linear subspaces and Hamming balls, and is particularly interesting. The statement of the structural theorem is</p> <blockquote> <p>If \(X_1, \ldots, X_\ell \in \mathbb{F}_2^n\) are vectors sampled independently and uniformly from a Hamming ball of radius \(\rho\), then the probability that more than \(C\ell\) vectors from the span of \(X_1, \ldots, X_\ell\) also lie in the Hamming ball is exponentially small (it is at most \(2^{-5n}\)).</p> </blockquote> <p>I will give an overview of how this proof technique works to make use of this structural theorem, and also give a short description of the proof of the structural theorem itself.</p> <h2 id="identifying-problematic-cases">Identifying problematic cases</h2> <p>Recall that the overall goal is to show that for a random linear code \(\mathcal{C}\) of rate \(1-h(\rho)-\epsilon\) and for list size \(L=10/\epsilon\), the probability of any bad event happening is very low. That is, the quantity: \[ \Pr_{\mathcal{C}}[ \exists x \in \mathbb{F}_2^n, |B_n(x, \rho) \cap \mathcal{C}| \geq L+1] \]</p> <p>should be very small, where \(B_n(x, \rho)\) is the set of all vectors which have Hamming distance at most \(\rho n\) from \(x\)). Glossing over some minor technical details, it suffices to show that \[ \Pr_{\mathcal{C}}[|B_n(0^n, \rho) \cap \mathcal{C}| \geq L+1] &lt; 2^{-\lambda n} \] for some positive constant \(\lambda\). That is, it is enough to show that the probability of a random linear code having too many codewords around the <em>origin-centered</em> Hamming ball is small. The same upper bound will then translate for any center \(x \in \mathbb{F}_2^n\), and so we can apply the union bound to get an upper bound for the first probability.</p> <p>Going back to the naive probabilistic proof sketched above, one immediately notices that we are playing it too safe by assuming the worst case scenario: that every set of \(L+1\) vectors will have a maximal linearly independent subset of size only \(\log(L+1)\). But most of the time, this is not true: most \(L+1\)-sized subsets will have many more linearly independent vectors within them. Separating these two cases and dealing with them differently allows us to achieve our goal.</p> <p>We first need to list out all the cases before separating them. Consider the family of all \(L+1\)-sized sets of vectors that appear within a \(\rho\)-radius Hamming ball centered at the origin. We need our random linear code to avoid containing every one of these ‘‘bad” sets within it. Divide this ‘‘bad” set family into two parts: one containing those \(L+1\)-sized sets of vectors such that the maxmimal linearly independent subset within it is of size at most \(L/4\), and the other containing ones having a maximal linearly independent subset of size greater than or equal to \(L/4\). These will be our two cases, and the former case is the more problematic one.</p> <h2 id="slight-digression-the-probabilistic-method">Slight digression: The Probabilistic Method</h2> <p>Recall the goal of the probabilistic method: given a collection of mathematical objects, use probabilistic techniques to prove that the collection contains at least one good mathematical object. (For concreteness, think of the collection as the set of all linear subspaces of \(\mathbb{F}_2^n\), and the good mathematical object as a linear subspace having good list-decoding properties, when viewing the subspace as an error-correcting code.) In the field of random (linear) codes, most probabilistic approaches follow one basic template: show that the probability of a bad event occurring is \(\epsilon\) for some tiny \(0 &lt;\epsilon &lt; 1\), then show that the number of bad events is strictly less than \(1/\epsilon\), and finally apply the union bound to get that the probability that none of the bad events occur is non-zero.</p> <p>I like to think of this particular probabilistic technique in the following manner: think of someone dropping objects from the top of a building, some objects are labeled as ‘bad’, and the rest of them are labeled ‘good’. The goal is to design a series of nets such that every bad object is caught in one of these nets. It is okay for some good objects to get caught as well, as long as at least one of them falls through each and every net, and hits the ground below. The bad events correspond to the nets, of course, and there is a balance that needs to be achieved in how they are designed. They might catch too many good objects if they are too big, or too numerous, but on the other hand they could allow some bad object to slip through if they are too weak.</p> <h2 id="dealing-with-the-cases">Dealing with the cases</h2> <p>As discussed a couple of sections ago, we will can rework the probability that we want to upper bound as: \[ \Pr_{\mathcal{C}}[|B_n(0^n, \rho) \cap \mathcal{C}| \geq L+1] \leq \Pr_{\mathcal{C}} [\mathcal{C} \text{ contains a bad set from the easy case}] + \] \[ \Pr_{\mathcal{C}}[\mathcal{C} \text{ contains a bad set from the hard case}] \]</p> <p>We will bound the two probabilities on the right hand side individually. If both of them are exponentially small, then we will be done.</p> <p>At this point, we note a fact which will come in handy while dealing with the two cases: for any fixed bad set, no matter what case it is from, the probability of a random linear code containing that bad set is upper bounded by the probability of a random linear code containing the maximal linearly indepedent subset within the bad set. This is because the latter event is a necessary event for the former one. Therefore, from here on we will only focus on upper bounding the latter event.</p> <p>Let us get the easy case out of the way first: the one where the bad set contains a lot of linear independent vectors (specifically, at least \(L/4\) elements in this bad set are linearly independent). In this case, upon using the handy fact above, we get that the probability that a random linear code of rate \(r\) simultaneously contains every vector from this bad set is at most \[ \left(\frac{2^{rn}}{2^n}\right)^{L/4}. \] Because the total number of bad sets is at most \((2^{h(\rho)n})^ {L+1}\), this number is also an upper bound on the number of bad sets in the easy case, and so upon union bounding over these easy-case-bad-sets, we get: \[ \Pr_{\mathcal{C}}[\mathcal{C} \text{ contains a bad set from the easy case}] \leq (2^{h (\rho)n})^{L+1} \cdot \left(\frac{2^{rn}}{2^n}\right)^{L/4} \leq 2^ {-\lambda n} \] for some positive constant \(\lambda\). Now onto the hard (and more interesting!) case.</p> <p>Because the probability of containing some bad set from the hard case is not necessarily low (something like \(2^{-cLn}\)), as it was in the easy case, we can’t use the naive upper bound on the number of hard-case-bad sets. We instead have to turn to investigate whether this quantity—the number of hard-case-bad sets—is low or not. It turns out that it is indeed low, and this is implied by the structural theorem we mentioned above! For any \(\ell\) such that \(\log (L+1) \leq \ell \leq L/4\), denote \(\mathcal{F}_\ell\) to be the number of hard-case-bad sets where the maximal number of linearly independent vectors is exactly \(\ell\). Then, because the \(C\) from the structural theorem above satisfies \(C&lt;4\) (take my word for this!), \(L\) is always greater than \(C\ell\) and so,</p> \[\frac{|\mathcal{F}_\ell|}{(2^{h(\rho)n})^{\ell}} \leq \Pr_{X_1, \ldots, X_\ell \sim B_n(0, \rho)} [|span(X_1,\ldots,X_\ell) \cap B_n (0^n, \rho)| \geq C \ell].\] <p>According to the theorem, the quantity on the right is at most \(2^ {-5n}\), and so we get: \[ |\mathcal{F}_\ell| \leq (2^{h(\rho)n})^{\ell} \cdot 2^{-5n} \]</p> <p>which further implies that:</p> \[\begin{align} \Pr_{\mathcal{C}}[\mathcal{C} \text{ contains a bad set from the hard case}] &amp;\leq \sum_ {\ell = \log (L+1)}^{L/4} |\mathcal{F}_\ell| \cdot \left(\frac{2^{rn}} {2^n}\right)^{\ell} \\ &amp;\leq 2^{-5n} \sum_{\ell = \log (L+1)}^{L/4} 2^{(h(\rho)+r-1)n\ell} \end{align}\] <p>If we take \(r\) to be equal to \(1-h(\rho)-\epsilon\), where \(\epsilon&gt;0\) is very small (recall that \(1-h(\rho)\) was the list-decoding capacity acheived by random codes, and our goal was to show that random linear codes with rate approaching \(1-h(\rho)\) are also list-decodable with the same parameters as random codes), we get that the last quantity is equal to:</p> \[2^{-5n} \sum_{\ell = \log (L+1)}^{L/4} 2^{-\epsilon n\ell}.\] <p>The term inside the summation is maximum when \(\ell=\log (L+1)\), and there are at most \(L/4\) terms in the summation, and so we get:</p> \[\begin{align} \Pr_{\mathcal{C}}[\mathcal{C} \text{ contains a bad set from the hard case}] &amp;\leq 2^ {-5n} \cdot \frac{L}{4} \cdot 2^{-\epsilon n\log (L+1)} \\ &amp;\leq 2^{-5n}. \end{align}\] <p>The only thing left is to prove the structural theorem.</p> <h2 id="proof-of-the-structural-theorem">Proof of the structural theorem</h2> <p>Here’s the statement of the structural theorem once more, so that you don’t have to scroll up to see it:</p> <blockquote> <p>If \(X_1, \ldots, X_\ell \in \mathbb{F}_2^n\) are vectors sampled independently and uniformly from a Hamming ball of radius \(\rho\), then the probability that more than \(C\ell\) vectors from the span of \(X_1, \ldots, X_\ell\) also lie in the Hamming ball is exponentially small (it is at most \(2^{-5n}\)).</p> </blockquote> <p>Intuitively, this is saying that if I pick a few vectors randomly from a Hamming ball and look at the subspace spanned by them, then with very high probability this subspace does not intersect too much with the Hamming ball.</p> <p>Denoting \(L := C\ell\), the proof is basically a union bound over some bad events. To describe these bad events, we need to first describe the set of all possible linear combinations of any \(\ell\) vectors from \(\mathbb{F}_2^n\). This is easy: every \(u \in \mathbb{F}_2^\ell\) will correspond to the following linear combination: \[ \sum_{i \in \ell} u(i) \cdot X_i \] where \(u(i)\) denotes the \(i\)th entry of \(u\). Now for every subset \(S \subset \mathbb{F}_2^{\ell}\) of size exactly \(L+1\), a bad event is one where every vector described by the linear combinations corresponding to all vectors from the tuple falls into the Hamming ball. That is, the following event is bad: \[ \forall u \in S \quad \sum_{i \in \ell} u(i) \cdot X_i \in B_n(0^n, \rho). \] There are \({2^{\ell} \choose L+1}\) bad events, one for each \(L+1\) sized subset of vectors from \(\mathbb{F}_2^\ell\), and so the only thing that’s left before we can apply the union bound is to estimate a good upper bound on the probability of each bad event. In pursuit of upper bounding the probability, we will choose to, for every \(L+1\)-sized subset \(S\) of \(\mathbb{F}_2^ {\ell}\), only focus on a special subset \(T \subset S\) of linear combinations. This special subset of vectors will have an ordering, and will have the property that for each vector \(u_i \in T\), there are at least two coordinates appearing in the support of \(u_i\) that do not appear in the support of all \(u_j, \forall j&lt;i\). Roughly, the reason why we choose to focus on this subset is because it will allow us to use the following neat result:</p> <blockquote> <p>(<em>Sum of two random vectors from zero-centered Hamming ball doesn’t fall into any fixed Hamming ball w.h.p.</em>): Let \(X_1\) and \(X_2\) be two vectors sampled independently from one another and uniformly at random from \(B_n (0,\rho)\). Then for <strong>any</strong> \(y \in \mathbb{F}_2^n\), the probability that \(X_1+X_2\) falls within \(B_n(y, \rho)\) is very low. In particular, it is at most \(2^{-\delta n}\), where \(\delta&gt;0\) is some small constant.</p> </blockquote> <p>The intuitive proof for this result is as follows: because \(X_1+X_2\) is essentially a uniformly random vector chosen from the set of all vectors having weight \(2p(1-p)=2p-2p^2\), it falls into \(B_n(0^n, \rho)\) with exponentially low probability. Also, we have: \[ \Pr[X_1+X_2 \in B_n(y,\rho)] \leq \Pr[X_1+X_2 \in B_n(0^n,\rho)]. \]</p> <p>We can thus proceed with the proof of the structural theorem now. The above discussion now implies:</p> <p>\[ \Pr[\forall u \in S \quad \sum_{i \in \ell} u(i) \cdot X_i \in B_n (0^n, \rho)] \leq \Pr[\forall u \in T \quad \sum_{i \in \ell} u (i) \cdot X_i \in B_n(0^n, \rho)] \] By a chaining arugment, the RHS is equal to: \[ \prod_{j=1}^{|T|} \Pr\left[\sum_{i \in \ell} u_j(i) \cdot X_i \in B_n (0^n, \rho) \mid \forall 1 \leq k &lt; j, \sum_{i \in \ell} u_k(i) \cdot X_i \in B_n(0^n, \rho)\right] \]</p> <p>But now we have set up everything perfectly in order for the neat result mentioned above to be used. Because of the property that the vectors describing linear combinations in \(T\) have an ordering, and futhermore are such that every vector has at least two coordinates in its support that does not appear in the previous vectors, we know that for every \(u_j\) there are two coordinates \(i_1\) and \(i_2\) in its support that are not in the support of any \(u_i\), for \(i&lt;j\). This allows us to write</p> <p>\[ \sum_{i \in \ell} u_j(i) \cdot X_i = u_j(i_1)\cdot X_{i_1} + u_j(i_2)\cdot X_{i_2} + \sum_{i \in \ell, i \neq i_1,i_2} u_j(i) \cdot X_i. \] This further allows us to rewrite that probability corresponding to \(u_j\) above as: \[ \Pr\left[u_j(i_1)\cdot X_{i_1} + u_j(i_2)\cdot X_{i_2} \in B_n \left(\sum_{i \in \ell, i \neq i_1,i_2} u_j(i) \cdot X_i, \rho\right) \mid \forall 1 \leq k &lt; j, \sum_{i \in \ell} u_k(i) \cdot X_i \in B_n(0^n, \rho)\right] \] Now because \(X_{i_1}\) and \(X_{i_2}\) are independent of the conditioning in the probability, the neat result is applicable and hence this probability is at most \(2^{-\delta n}\), and so the product of probabilities is at most \(2^{-\delta n |T|}\).</p> <p>This was the probability of a single bad event occurring. Recall that there were \({2^{\ell} \choose L+1}\) bad events, one for each \(L+1\) sized subset of vectors from \(\mathbb{F}_2^\ell\). Now \(|T|\) is large enough so that \(2^{-\delta n |T|}\) is small enough, and this in turn ensures that: \[ 2^{-\delta n |T|} \cdot {2^{\ell} \choose L+1} \leq 2^{-5n}. \] This completes the proof for the structural theorem.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Combinatorial Bounds for List Decoding: <a href="https://people.csail.mit.edu/madhu/papers/2000/ghsz-conf.pdf">https://people.csail.mit.edu/madhu/papers/2000/ghsz-conf.pdf</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>Improved list-decodability of random linear binary codes: <a href="https://arxiv.org/abs/1801.07839">https://arxiv.org/abs/1801.07839</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>On the List-Decodability of Random Linear Codes <a href="https://arxiv.org/abs/1001.1386">https://arxiv.org/abs/1001.1386</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="coding-theory"/><category term="exposition"/><summary type="html"><![CDATA[Introduction List-decodability (at least the combinatorial aspect of it) is concerned with producing codes where only a few codewords fall within any given Hamming ball of small radius. A code \(\calC \subseteq \mathbb{F}_2^n\) is said to be list-decodable with list size \(L\) and radius \(\rho\) if at most \(L\) codewords lie within every Hamming Ball in \(\mathbb{F}_2^n\) of radius \(\rho \cdot n\). A natural question to ask about list-decodable codes is: for a fixed (normalized) radius \(\rho\) and list size \(L\), what is the maximum rate that a \((\rho, L)\) list-decodable code can have? A simple computation proves that it is at most \(1-h(\rho) + o(1)\), where \(h: [0, 1] \rightarrow [0, 1]\) is the binary entropy function. The quantity \(1-h(\rho)\) is also known as the list-decoding capacity.]]></summary></entry><entry><title type="html">Mergers from Kakeya Sets</title><link href="https://nikhilshagri.github.io/blog/2023/mergers/" rel="alternate" type="text/html" title="Mergers from Kakeya Sets"/><published>2023-04-22T00:00:00+00:00</published><updated>2023-04-22T00:00:00+00:00</updated><id>https://nikhilshagri.github.io/blog/2023/mergers</id><content type="html" xml:base="https://nikhilshagri.github.io/blog/2023/mergers/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Consider the following problem: given \(k\) probability distributions, with the guarantee that at least one of them is the uniform distribution, the task is to come up with a procedure that ‘merges’ these distributions and obtains a distribution which is uniform. We do not know which of the \(k\) distributions is uniform, and to make things interesting, the other distributions can be correlated among themselves, and also with the uniform distribution. Moreover, this ‘merging’ procedure is required to be ‘blind’ in a certain sense, meaning that we are not providing it with any information regarding the probabilities associated with the inputs.</p> <p>What does the last sentence mean, really? It becomes clearer once we formalize the above paragraph. Taking our sample space to be \(\{0, 1\}^n\), we require the function (we’ll call it a merger henceforth) to be of the following form: \[ f: \{0, 1\}^{n \times k} \rightarrow \{0, 1\}^n, \] and also satisfy the requirement that the function outputs every string in \(\{0, 1\}^n\) with equal probability, given that the inputs provided to the function are sourced from the distributions described in the previous paragraph. More formally, for every \(y \in \{0, 1\}^n\), the sum of the probabilities of those inputs in the pre-image of \(y\) has to add up to \(2^{-n}\). In mathematical notation: \[ \forall y \in \{0, 1\}^n \sum_{x \in f^{-1}(y)} Pr[X=x] = 2^{-n} \] Hopefully, the function signature should makes the last statement in the first paragraph clear. The function will be given a bit string of length \(n \times k\), and has to output a bit string of length \(n\). It does not have any information about the probability with which the provided input is being sampled with. This might seem like a daunting task at first sight, but it has a surprisingly elegant solution, which I will describe in this blog post.</p> <p>Now to be completely honest, I lied in a few places in the definition above. Firstly, to ask for the output distribution to be strictly uniform on \(n\) bits is very restrictive, and we can relax this condition in two ways: first, instead of asking for the output distribution to be uniform, we can ask for it be a high min-entropy distribution. A distribution on \(\{0, 1\}^n\) has min-entropy \(t\) for some \(t \leq n\) if every string is sampled with probability at most \(1/2^t\). It is easy to see that the uniform distribution has min-entropy equal to \(n\), and also that intuitively, greater the min-entropy of a distribution, greater the amount of ‘randomness’ contained within it. Second, we can ask for the output distribution to be close in statistical distance to some high min-entropy distribution, instead of asking for it to be exactly equal to one. This relaxation is OK, because every Boolean function still ‘behaves’ similarly on a statistically-close-to-uniform distribution, to as it would on a uniform distribution.</p> <p>Lastly, we need a small amount of additional randomness for the function to work, and it is easy to see that any deterministic function cannot be a good merger. To see this, consider just two sources, and the output to be just one bit. Even in this easy special case, any deterministic function of the form \[ f: {0, 1}^n \times \{0, 1\}^n \rightarrow \{0, 1\} \] will not work. That is, for any given \(f\), there exist distributions \(X_1, X_2\) on \(\{0, 1\}^n\), with one of them being uniform, such that the output of \(f\) will be far from uniform. It is left as an exercise to the reader to work out for themselves why this is true!</p> <h2 id="kakeya-sets">Kakeya Sets</h2> <p>In the world of finite fields, a Kakeya set is a subset of \(\mathbb{F}_q^n\) containing a line in every direction, up to translation. Precisely, a susbet \(K \subseteq \mathbb{F}_q^n\) is a Kakeya set if for every element \(\mathbb{F}_q^n\), there exists an element \(y \in \mathbb{F}_q^n\) such that \[ \forall a \in \mathbb{F}_q, y + ax \in K. \]</p> <p>An interesting question to ask is: what is the smallest possible size of a Kakeya set? A lower bound of \(q^{n/2}\) is easy to see: every Kakeya set has the useful property that the set \[ S := \{x - y : x, y \in K\} \] fills up the entire space of \(\mathbb{F}_q^n\). This is because following the definition of a Kakeya set, for any vector \(z \in \mathbb{F}_q^n\), there exists a \(y \in \mathbb{F}_q^n\) such that both \(y \in K\), and \(y + z \in K\). Therefore, we get \(z\) by subtracting \(y\) from \(y+z\). Now, one can see that if \(S\) has to contain \(q^n\) vectors, then \(S\) has to satisfy: \[ q^n \leq |S|, \] and we use the fact that \[ |S| \leq |K|^2 \] to get what we want. The last inequality is true because there are \(|K|^2\) ways to choose a tuple \((x, y) \in K^2\).</p> <p>But what do Kakeya sets have to do with mergers? Dvir and Wigderson in <a href="https://www.cs.princeton.edu/~zdvir/papers/DvirWigderson08.pdf">this paper</a> come up with a surprisingly simple construction for a merger, whose analysis relies on analyzing ‘approximate’ Kakeya sets. This merger construction allows them to plug it into another construction of a much more powerful pseudorandom object called an extractor, and results in an extractor with optimal parameters. We won’t talk about extractors here, but <a href="https://people.seas.harvard.edu/~salil/pseudorandomness/extractors.pdf">this chapter</a> from Vadhan’s book is a great introduction to the topic.</p> <p>The merger construction works entirely over the field \(\mathbb{F}_q\), for an appropriately chosen \(q\). We will represent the binary strings from \(\{0, 1\}^n\) as vectors from \(\mathbb{F}_q^r\) (where \(r\) will be a divisor of \(n\)) and work with them, and converting back to the binary representation once we have the output. ‘Lifting’ up the binary strings to a higher alphabet allows us to use the properties of the ‘approximate’ Kakeya sets in order to analyze the behaviour of the merger.</p> <h2 id="merger-construction">Merger Construction</h2> <p>I won’t list out all the parameters here, or the reasoning behind their values, since the original paper already has them, and moreover, there’s a lot of them. Instead, I will only mention the essential ones, and try to give some intuition on how the merger behaves. We take \(q\) roughly to be equal to \(nk\), recalling that \(k\) is the number of sources. We will work with the field \(\mathbb{F} := \mathbb{F}_q\). For simplicity, we will also take \(n\) to be a multiple of \(\log_2 q\), because then we can represent any \(n\)-bit binary string as a vector from \(\mathbb{F}^r\), where \(r:= n/\log_2 q\). From here on, we can think of the merger \(M\) as a function of the form \(M: (\mathbb{F}^r)^k \rightarrow \mathbb{F}^r\) (compare this to \(f\)’s signature in the introduction). Lastly, we will keep with the convention of using capital letters to denote random variables, and small letters to denote the values they take.</p> <p>\(M\) is defined as: \[ M(X_1, \ldots, X_k, U) := c_1(U)\cdot X_1 + \ldots + c_k(U)\cdot X_k \] where each \(X_i\) is some distribution on \(\mathbb{F}^r\) (that is, after lifting it from a distribution on \(\{0, 1\}^n\)), \(U\) is a random variable uniformly distributed on \(\mathbb{F}\) which represents the additional randomness that the merger requires, and each \(c_i(.)\) is a univariate polynomial from the ring \(\mathbb{F}[u]\). These polynomials are an indicator function of sorts: we shall first fix elements \(\gamma_1, \ldots, \gamma_k\) from \(\mathbb{F}\), and then construct the polynomials such that \(c_i(u)\) will be equal to \(1\) when \(u=\gamma_i\), will be equal to \(0\) when \(u=\gamma_j\) for \(j \neq i\), and will take on some value from \(\mathbb{F}\) when the input is not any of \(\gamma_1, \ldots, \gamma_k\).</p> <p>Aside from the above description, the only other fact that we will use about the polynomials \(c_1, \ldots, c_k\) is that they all have degree at most \(k-1\); but if you’re still curious about them, here’s how they look: \[ c_i(u) := \frac{\prod_{j \neq i} (\gamma_j - u)}{\prod_{j \neq i}(\gamma_j - \gamma_i)} \]</p> <h2 id="high-level-idea">High Level Idea</h2> <p>The claim is that the merger’s output distribution (which is a distribution over \(\mathbb{F}^r\)) is close to some high min-entropy distribution. For the sake of contradiction, we will assume that it is far from every high-min entropy distribution, which implies the existence of a small set \(T \subset \mathbb{F}^r\) on which a large probability mass is placed by the merger’s output distribution. The high level idea is to show that this set is some approximate form of a Kakeya set in \(\mathbb{F}^r\), and therefore it cannot be too small, thus arriving at the contradiction.</p> <p>To show that this set \(T\) cannot exist, we will use the polynomial method, along the lines of <a href="https://www.cs.princeton.edu/~zdvir/papers/Dvir09.pdf">Dvir’s proof</a> for the lower bound on the size of a Kakeya set in \(\mathbb{F}^n\). This will involve</p> <ol> <li>proving that existence of \(T\) implies the existence of a non-zero polynomial \(g: \mathbb{F}^r \rightarrow \mathbb{F}\) of low degree which completely vanishes on \(T\) (meaning that for every \(t \in T, g(t)=0\)),</li> <li>constructing a set \(G \subseteq \mathbb{F}^r\) whose size is much larger than the size of \(T\),</li> <li>showing that \(g\) vanishing on \(T\) implies that \(g\) must necessarily vanish on \(G\) as well.</li> <li>This will imply that either \(g\) is the zero polynomial, or has high degree, both of which are not true. Therefore, this implies such a \(g\) does not exist, which in turn implies that such a \(T\) does not exist.</li> </ol> <p>In the next section, we will show why the merger’s output distribution being far away from every high-min entropy distribution implies that such a small set \(T\) must exist, and in the subsequent sections, we will go over each of the steps in detail.</p> <h2 id="existence-of-small-set-t">Existence of small set \(T\)</h2> <p>WLOG, we will assume that \(X_1\) is the uniform distribution. The claim is that for every set of distributions \(X_1, \ldots, X_k\) where \(X_1\) is uniform on \(\mathbb{F}^r\), the merger’s output distribution is \(\epsilon\)-close to some distribution having min-entropy \((1-\alpha)r\), where \(\alpha\) is a very small constant. For contradiction, assume that this merger promise does not hold, i.e, there is some set of distributions \(X_1, \ldots, X_k\) with \(X_1\) uniform, and yet the merger’s output distribution is at least \(\epsilon\)-far from every distribution of min-entropy \((1-\alpha)r\). This implies that there is a small set \(T \subset \mathbb{F}^r\) of size at most \(q^{(1-\alpha)r}\) such that the merger’s output places a substantial probability mass on it. Formally: \[ \Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) \in T] \geq \epsilon. \]</p> <p>The existence of such a \(T\) can be shown via a proof of contradiction as well. Therefore, assume that for every set \(S \subset \mathbb{F}^r\) where \(|S| \leq q^{(1-\alpha)r}\), the following holds: \[ \Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) \in S] &lt; \epsilon. \] Now, consider all elements that are ‘witnesses’ to the fact that \(M(X_1, \ldots, X_k, U)\) does not have min-entropy \((1-\alpha)r\), that is, all \(x \in \mathbb{F}^r\) such that \(\Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) = x] &gt; q^{(1-\alpha)r}\). There can only be at most \(q^{(1-\alpha)r}\) such elements, otherwise their combined probabilities will exceed one. Now, because of our assumption, this set of elements has at most \(\epsilon\) probability mass on it. If we move away all of the mass to elements which have very low mass, then the new distribution that we have created has min-entropy at least \((1-\alpha)r\) (because we have ‘eliminated’ all of the ‘witnesses’), and we only moved \(\epsilon\) mass in doing so, therefore the new distribution is \(\epsilon\)-close to the new distribution. This contradicts the fact that the original distribution is \(\epsilon\)-far from every distribution having min-entropy at least \((1-\alpha)r\).</p> <h2 id="existence-of-polynomial-g">Existence of polynomial \(g\)</h2> <p>In this section, we show the existence of a polynomial \(g: \mathbb{F}^r \rightarrow \mathbb{F}\) of total degree at most \(d := q^{1-\alpha}\), which completely vanishes on \(T\). Firstly, note that the number of \(r\)-variate monomials having degree at most \(d\) is equal to: \[ {r+d \choose r}. \] One way to see this is by placing \(d\) marbles in a row, and counting the number of ways \(r\) sticks can be placed in between the marbles, or at either end. Here is one possible arrangement for \(d=7, r=4\):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sticks-stones-480.webp 480w,/assets/img/sticks-stones-800.webp 800w,/assets/img/sticks-stones-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/sticks-stones.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Each such arrangement corresponds to a monomial whose total degree is less than \(d\). The number of marbles to the left of the \(i\)th stick, and to the right of the \(i-1\)th stick is equal to the degree of variable \(x_i\). Therefore, the above arrangement corresponds to the monomial \(x_1^0x_2^0x_3^2x_4^1 = x_3^2x_4\).</p> <p>Finally, the value for \(r\) has been cleverly chosen such that \[ {r+d \choose r} \geq |T| \] is true. This implies that there is a non-zero polynomial \(g: \mathbb{F}^r \rightarrow \mathbb{F}\) having total degree at most \(d\) that vanishes on \(T\). This is easily seen by solving for \(Ax=0\), where \(A\) is a matrix of dimension \(|T| \times {r+d \choose r}\), with each column corresponding to a monomial, and the entries being the evaluations of the monomial on points from \(T\). This is a matrix having full rank, and therefore one can find an \(x\) satisfying \(Ax=0\). The entries of \(x\) will determine the coefficients of \(g\), the polynomial whose existence we wanted to show.</p> <h2 id="constructing-the-much-larger-vanishing-set-g">Constructing the (much) larger vanishing set \(G\)</h2> <p>Where do we even begin? We don’t know anything about the structure of the polynomial \(g\). All we know is that it is a polynomial of low-degree which vanishes on \(T\). At first glance, it might seem like there isn’t much to work with, if we want to build a larger vanishing set for \(g\), but the unique structure of \(T\) will come to our rescue.</p> <p>We will consider all elements \(x_1 \in F^r\) such that if we condition on the event \(X_1 = x_1\), then the merger’s output will fall into \(T\) with high probability. That is, the following set \(G\): \[ G := \{ x_1 \in \mathbb{F}^r \mid \Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) \in T \mid X_1 = x_1] \geq \epsilon/2 \}. \] We will show that for every \(x_1 \in G\), \(g(x_1)=0\).</p> <p>At this point, the observant reader should be thinking: how is it that an element that with high probability over the other random variables takes \(M\)’s output to \(T\), also makes \(g\) vanish when we give that element as an input to \(g\)? This question will be answered duly, but first we must state that the size of \(G\) is large: \[ |G| \geq \frac{\epsilon}{2}q^r. \] This follows from a simple averaging argument, but I won’t go into it here. Lastly, one interesting fact to note is that \(T\) was defined as a subset of the range of \(M\), while \(G\) was defined as a subset of the space corresponding to the first input of \(M\).</p> <h2 id="g-vanishing-on-t-implies-g-vanishing-on-g">g vanishing on \(T \implies g\) vanishing on \(G\)</h2> <p>We mentioned in the previous section that \(T\) has some unique structure, which is somewhat like an approximate Kakeya set. We make this notion precise in this section.</p> <p>Firstly, recall that by the definition of \(G\), for every \(x_1 \in G\), we have \[ \Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) \in T \mid X_1 = x_1] \geq \epsilon/2. \] This implies that there exist values \(x_2, \ldots, x_k \in \mathbb{F}^r\) that we can assign to the other random variables \(X_2, \ldots, X_k\) such that we get: \begin{equation} \Pr_{X_1, \ldots, X_k, U}[M(X_1, \ldots, X_k, U) \in T \mid X_1 = x_1, X_2=x_2,\ldots,X_k=x_k] \geq \epsilon/2. \end{equation} At this point, the only remaining randomness is coming from \(U\), which was the random variable used as input to the polynomials \(c_i(.)\), in the construction of the merger.</p> <p>This means that after we have fixed an \(x_1 \in G\) and also fixed the corresponding \(x_2, \ldots, x_k\) as specified above, the merger function is of the form \(M_{x_1, \ldots, x_k}: \mathbb{F} \rightarrow \mathbb{F}^r\), where the only variable left is the input to the polynomials \(c_i(.)\). The subscript of \(x_1, \ldots, x_k\) for \(M\) is used to denote that those inputs to \(M\) have already been fixed. Now, observe from the above inequality that \(T\) contains an \(\epsilon/2\) fraction of points from the image of the function \(M_{x_1, \ldots, x_k}\).</p> <p>We are now in a position to state how \(T\) is like an approximate Kakeya set: because \(G\) contains an \(\epsilon/2\) fraction of points in \(\mathbb{F}^r\), we can carry out the above procedure for every \(x_1 \in G\), and create a corresponding function \(M_{x_1, \ldots, x_k}\) such that an \(\epsilon/2\) fraction of the points from the image of the function fall into \(T\). Compare this to a natural definition of an approximate Kakeya set: a set containing a large fraction of points for a large fraction of directions in the vector space. Our set \(T\) is a generalized version of this, containing a large fraction of points for a large fraction of functions whose image is a subset of the vector space.</p> <p>We turn to the main task of this section: to prove that \(g(x_1)=0\) for every \(x_1 \in G\). Given any \(x_1 \in G\), we first construct \(M_{x_1, \ldots, x_k}(.)\), and then define \(h(u) := g(M_{x_1, \ldots, x_k}(u))\), as detailed above. Recall that by its definition, \(h\) has the form \(h: \mathbb{F} \rightarrow \mathbb{F}^r\). The definition of \(h\) might seem a bit strange: we want to prove the \(g(x_1)=0\), where \(x_1\) is the first input to the merger, but we have defined \(h\) by passing the output of the merger to \(g\). How is this going to be of any help? The trick is noticing that \(h(\gamma_1) = g(x_1)\), by how the merger was defined. We will now prove that \(h(\gamma_1)=0\), which will give us what we require. In fact, we will prove something much stronger: \(h(u)\) is identically zero for all inputs \(u\)!</p> <p>We have already done the bulk of the work to show this result, actually. The last inequality written above states that for an \(\epsilon/2\) fraction of \(u \in \mathbb{F}\), \(h(u)=g(M(x_1, \ldots, x_k, u))=0\). This follows because \(g\) is zero on every point in \(T\). Therefore, we get that \(h\) is zero on an \(\epsilon/2\) fraction of its inputs. But from the definition of \(h\) we can see that it is just a degree \(k-1\) polynomial in \(\mathbb{F}\). Because we have chosen \(q\) to be equal to \(nk\), the Schwartz-Zippel lemma says that the only way \(h\) can \(\epsilon/2\cdot q\) zeroes is if it was the zero polynomial, or if it had degree \(q\). We know that it has degree strictly less than \(q\), and therefore it has to be the zero polynomial, and hence \(h(u)\) is identically zero for all inputs \(u\).</p> <h2 id="conclusion">Conclusion</h2> <p>The result proved in the above section in turn implies that again, by the Schwartz-Zippel lemma, because \(g\) is zero on \(\epsilon/2\cdot q^r\) inputs, it must either be the zero polynomial, or have degree \(q\). But this contradicts the fact that \(g\) was a nonzero polynomial of degree \(q^{1-\alpha} &lt; q\) to begin with, and therefore, such a \(g\) could not have existed. Because the existence of the small set \(T\) implied the existence of \(g\), by contrapositive, such a small set \(T\) could not have existed as well, and therefore, the output of the merger is not concentrated within any small set, meaning that it is close to some high min-entropy distribution.</p> <p>Lastly, the construction is general enough to also work if there is no uniform distribution among the inputs. In this case, the merger’s output distribution will be close to some distribution having min-entropy roughly as much as the maximum of the min-entropies of the input distributions.</p>]]></content><author><name></name></author><category term="pseudorandomness"/><category term="exposition"/><summary type="html"><![CDATA[Introduction Consider the following problem: given \(k\) probability distributions, with the guarantee that at least one of them is the uniform distribution, the task is to come up with a procedure that ‘merges’ these distributions and obtains a distribution which is uniform. We do not know which of the \(k\) distributions is uniform, and to make things interesting, the other distributions can be correlated among themselves, and also with the uniform distribution. Moreover, this ‘merging’ procedure is required to be ‘blind’ in a certain sense, meaning that we are not providing it with any information regarding the probabilities associated with the inputs.]]></summary></entry><entry><title type="html">Oracle separation of PSPACE and PH</title><link href="https://nikhilshagri.github.io/blog/2019/oracle-sep/" rel="alternate" type="text/html" title="Oracle separation of PSPACE and PH"/><published>2019-12-01T00:00:00+00:00</published><updated>2019-12-01T00:00:00+00:00</updated><id>https://nikhilshagri.github.io/blog/2019/oracle-sep</id><content type="html" xml:base="https://nikhilshagri.github.io/blog/2019/oracle-sep/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>An important aspect of complexity theory is to try and find separations between different complexity classes. That is, to show that two complexity classes are not equal. One type of separation is a strict containment. It turns out that it is easier to prove containments (for example, give two complexity classes \(A\) and \(\), to show that \(A \subseteq B\), or the opposite, whichever one is true) but to show a strict containment (\(A \subset B\)) is more difficult (\(\mathbf{P}\) vs \(\mathbf{NP}\) being the best example of this). Obviously, a strict containment implies a separation between the two classes, but a containment doesn’t.</p> <p>So if strict containments are harder to prove, what can we do? One strategy is to try to solve a simpler, related problem: oracle separations. Oracle separations do not imply an actual separation between classes, but one can think of them as a ‘necessary condition’ for it. That is, if a separation indeed exists between two complexity classes, then there definitely exists an oracle separation between them.</p> <p>The paper I’m going to describe in this post shows an oracle separation between the classes \(\mathbf{PH}\) and \(\mathbf{PSPACE}\), by proving a lower bound on a certain type of boolean circuits, which was a completely original idea at that time. Since then, others have used the idea to show many other interesting results too.</p> <h2 id="main-result">Main result</h2> <p>The main result which is going to be discussed here is that if the \(\mathbf{AC^0}\) circuit (polynomial sized <a href="https://en.wikipedia.org/wiki/Boolean_circuit">boolean circuits</a> with constant depth, unbounded fan-in) which computes the parity function has size \(\Omega(n^{log^k(n)})\), then there exists an oracle \(\mathbf{O}\) relative to which \(\mathbf{PH^O} \subset \mathbf{PSPACE^O}\). The paper also goes on to show that the parity function indeed requires a \(\Omega(n^{log^k(n)})\) sized circuit to compute, but the proof shown in this paper is relatively complicated. I will probably do another post discussing an alternative proof, which uses Hastad’s Switching Lemma.</p> <h2 id="complexity-theory-a-primer">Complexity theory: a primer</h2> <p>As the name suggests, complexity theory is about the study of ‘complexity’ of problems. Problems like sorting, shortest path, travelling salesman, or SAT. So how do we define how ‘complex’ a problem is? We need a quantifiable measure, in order to compare it with other problems. One measure is the time taken to correctly compute the solution, given some input. But think about the variables involved here. Surely an algorithm which brute forces its way to the solution, when executed on a speedy brand new processor will execute in less time than a super efficient algorithm running on some 90s era Pentium chip? And what about the language in which we have written the algorithm in? And the compiler used? Would forgetting to specify the <code class="language-plaintext highlighter-rouge">-O3</code> flag during compilation somehow increase the complexity of the program, because it takes more time to execute?</p> <p>To ‘normalize’ away all these differences, we consider the number of steps taken by the ‘fastest, correct’ algorithm to be the time complexity of the problem. The number of steps required would be a function of the input size, which is usually some natural number, denoted by \(n\). One way of looking at the input size is the number of bits required to completely specify the input for the problem.</p> <p>Also, intuitively one can deduce the fact that the number of steps required will increase as the input size increases. Which means we can represent the time complexity of problems using monotonically increasing functions.</p> <p>Complexity theory is more generally concerned with grouping problems of similar complexities together, so that it is easier to characterize them, and compare them with other problems. These groups are what we call complexity classes. One such class, referred to as \(\mathbf{P}\), includes all such problems having time complexity which look like \(n^2\), or \(n^3\), or \(n^4\)… multiplied by a fixed constant. Generally, this fixed constant is the normalization part, and is generally ignored while talking about a problem’s complexity. Another class, known as \(\mathbf{NP}\), is the class of efficiently ‘verifiable’ problems.</p> <p>For a better understanding of the \(\mathbf{P}\) and \(\mathbf{NP}\) classes, take a look at these <a href="http://people.cs.georgetown.edu/~cnewport/teaching/cosc240-fall18/psets/complexity-notes.pdf">lecture notes</a> which cover them, and provide their formal definitions alongside.</p> <p>Note: One point which the notes don’t mention is that for every string in a language which belongs to \(\mathbf{NP}\), the corresponding certificate is of size polynomial in the string’s length.</p> <h3 id="conp">coNP</h3> <p>A language’s (say, \(L\)) complement (denoted by \(\overline{L}\)) is the set of all strings which don’t belong to \(L\). Suppose \(L\) is the set of all graphs (or more precisely, the binary representation of such graphs) which have a <a href="https://en.wikipedia.org/wiki/Vertex_cover">vertex cover</a> of size at most \(k\). Then \(\overline{L}\) consists of all strings other than those present in \(L\). It will mostly contain binary strings which do not have a valid interpretation as a graph, and among the valid strings, the corresponding graphs will have vertex covers of size more than \(k\).</p> <p>We say that \(\overline{L}\) belongs to \(\mathbf{coNP}\) if \(L\) belongs to \(\mathbf{NP}\). The most famous example of a language in \(\mathbf{coNP}\) is \(\overline{SAT}\), which consists of all boolean formulae not having even a single satisfying assignment ((\(x_1 \wedge \overline{x_1})\) being an example). Languages belonging to \(\mathbf{coNP}\) don’t have a short certificate (that we know of) that provides a ‘proof of membership’, as in the case of \(\mathbf{NP}\). The formal definition of \(\mathbf{coNP}\) is as follows:</p> \[x \in L \iff \forall u \text{ } M(x, u) = 1\] <p>Where \(L\) is a language belonging to \(\mathbf{coNP}\), and \(u\) is a variable meant to represent all possible certificates.</p> <h3 id="pspace">PSPACE</h3> <p>This complexity class encapsulates the problems which are solvable by a machine with a polynomial amount of space. There’s no restriction on the time taken to solve the problem, as long as the number of steps taken is finite. It is easy to see how this class encapsulates \(\mathbf{NP}\): simulate a verifier for the problem, cycle through all possible certificates (its length at most polynomial in the input size, so given an input of size \(n\), there exist at most \(2^{p(n)}\) certificates, where \(p\) is some univariate polynomial), and in each cycle, feed the certificate and the input to the verfier. If the verifier accepts for a particular certificate, then accept. Else, if it cycles through all possible certificates and doesn’t accept on any of them, reject.</p> <h3 id="polynomial-hierarchy-ph">Polynomial Hierarchy (PH)</h3> <p>This class, often denoted by \(\mathbf{PH}\), encompasses those problems which the classes \(\mathbf{NP}\) and \(\mathbf{coNP}\) are unable to characterize. For instance, consider the \(MAX-INDSET\) problem. Given a graph, an independent set is a subset of vertices such that no two vertices in the subset are adjacent to each other. The \(MAX-INDSET\) problem is concerned with finding the largest independent set for a given graph.</p> <p>Now, consider the related decision problem. Note that the input has two parts: a description of graph G, and an integer k:</p> \[INDSET = \{ \langle G, k \rangle: \text{G has an independent set of size at least k}\}\] <p>There exists a short certificate for every valid solution to this problem, namely, the independent set itself. Verifying it is a simple matter of checking that it is of size at least \(k\), and that it forms an independent set, both of which can be done in polynomial time. Therefore this problem lies in \(\mathbf{NP}\).</p> <p>We can go even further and devise a modification of this problem:</p> \[\text{EXACT-INDSET} = \{ \langle G, k \rangle: \text{The largest independent set (LIS) of G has size exactly k}\}\] <p>It is not clear whether the definitions of \(\mathbf{NP}\) or \(\mathbf{coNP}\) are enough to characterize this problem. We can find another way to formulate the same problem, though:</p> <p>\(\text{EXACT-INDSET} = \{ \langle G, k \rangle:\) There exists an independent set of size k, and every subset which is of size more than k is not an independent set \(\}\)</p> <p>More formally, this can be written as:</p> \[\text{EXACT-INDSET} = \{ x := \langle G, k \rangle: \exists u\text{ } \forall v \text{ s.t. } M(x, u, v) = 1 \}\] <p>Where \(u\) represents the independent set of size \(k\), and \(v\) denotes a subset of vertices of size more than \(k\), and \(M\) is a function denoting a polynomial time Turing machine computation (it’s ok if you don’t know what a Turing Machine is, just think of it as a formal definition of an ordinary computer system).</p> <p>Now, we must ensure that \(M\), given an input \(x\), along with \(u\) and \(v\) as defined, carries out a computation which captures the essence of the problem. This can be done if we configure \(M\) to compute the following things (note that all these operations can be done in polynomial time. This is important, the fact that M is a polynomial time machine: it’s where the complexity class derives its name from!) :</p> <ul> <li>Ensure that \(u\) is of size \(k\), and that it is indeed an independent set.</li> <li>Similarly, ensure that \(v\) is of size more than \(k\), and that it is not an independent set.</li> <li>If for a given \(x\), \(u\), \(v\), the two conditions mentioned above are satisfied, then output \(1\). Else, output zero.</li> </ul> <p>We can create another program which given input \(x\), and descriptions of the variables \(u\), \(v\), and a description of the behaviour of \(M\), will cycle through all possible combinations of \(u\) and \(v\), running the machine \(M\) for each combination. and if it finds a particular \(u\) such that when combined with all possible \(v\), \(M\) outputs 1 every single time, then we have found our solution.</p> <p>Else, if the LIS isn’t of exactly of size \(k\), either of two cases are possible. The first case is that the LIS has size strictly lesser than \(k\). In this event, the outer program won’t be able to find a suitable candidate for \(u\) at all, and the inner program will return zero each time. The second case is when the LIS has size strictly greater than \(k\), whereupon we are guaranteed that for some \(v\) corresponding to that LIS, and for some subset of \(v\) of size equal to \(k\), the inner computation will return zero. Thus we have defined a framework which completely characterizes the \(EXACT-INDSET\) problem. (Note that I have used the terms program, computation and machine interchangeably).</p> <p>It turns out that that this framework is applicable to many problems, and we have a name for it: \(\Sigma_2^p\). The \(\Sigma\) indicates that the first quantifier is existential, the \(2\) indicating that we use two quantifiers, and the \(p\) simply saying that we are only using polynomial time computations (this does <strong>not</strong> mean that the problem is computable in polynomial time! Observe that we use an <em>exponential number</em> of polynomial time computations to characterize the input).</p> <p>We can define the class \(\Pi_2^p\) similarly. For any language \(L \in \Pi_2^p\):</p> \[x \in L \iff \{\forall u \text{ } \exists v \text{ s.t. } M(x, u, v) = 1 \}\] <p>The sizes of variables associated with the quantifiers that we’ve defined up until this point (and will define later) are polynomial in the size of the input.</p> <p>An example of a problem in \(\Pi_2^p\) is the \(MIN-CNF\) problem: given a <a href="https://en.wikipedia.org/wiki/Conjunctive_normal_form">3-CNF</a> boolean formula (each clause having three variables with two \(OR\) operators, the clauses themselves acted upon by \(AND\) operators), find an equivalent boolean formula having fewer clauses.</p> <p>This problem can be restated as follows: A 3-CNF boolean formula (more precisely, its string representation) lies in \(MIN-CNF\) iff for every 3-CNF boolean formula having more clauses, there exists an assignment of inputs for which the two formulas give different outputs (that is, the two formulas are not equivalent). It is easy to see that \(MIN-CNF \in \Pi_2^p\).</p> <p>We can now generalize the notion of \(\Pi_2^p\) and define the class \(\Pi_i^p\). For any language \(L \in \Pi_i^p\), the following holds:</p> \[x \in L \iff \{\forall u_1 \text{ } \exists u_2 \text{ } \forall u_3 \ldots Q_i u_i \text{ s.t. } M(x, u_1, u_2, u_3, \ldots, u_i) = 1 \}\] <p>Where \(Q_i\) is an existential or a universal quantifier depending on whether \(i\) is even or odd. We can define \(\Sigma_i^p\) in a similar manner. Note that the quantifiers are alternating with one another, because if we had two quantifiers of the same type adjacent to one another, it is always possible to combine them.</p> <p>The complexity classes generated by the generalizations have some very neat relationships among them (they apply to all \(i&gt;0\)):</p> <ol> <li>\(\Pi_i^p \subseteq \Pi_{i+1}^p\) (similarly for \(\Sigma_{i}^p\))</li> <li>\(\Sigma_i^p \subseteq \Pi_{i+1}^p\) (similarly for \(\Pi_{i}^p\))</li> <li>\(\Pi_i^p = co\Sigma_i^p\) (uses the fact that negation of an existential quantifier is a universal quantifier, and vice versa).</li> </ol> <p>And finally, after all these definitions, we arrive at the complexity class we’ve all been waiting for!</p> \[\mathbf{PH} = \bigcup_{i} \Sigma_i^p = \bigcup_{i} \Pi_i^p\] <p>Okay, that was a bit anti-climatic. But \(\mathbf{PH}\) is an interesting class to study, as we shall soon see. The classes can be visualized in the form of a ladder. An arrow from a class \(A\) to class \(B\) indicates a containment (that is, \(A \subseteq B\)):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ph-400-480.webp 480w,/assets/img/ph-400-800.webp 800w,/assets/img/ph-400-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/ph-400.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>(\(\Delta_i^p\) is just the intersection of \(\Sigma_{i+1}^p\) and \(\Pi_{i+1}^p\), but we don’t need to be concerned with those.)</p> <p>The most amazing thing about \(\mathbf{PH}\) is that we don’t know whether <em>any</em> of these containments are strict or not (as I mentioned earlier, we are very bad at proving/disproving strict containments, and many believe we haven’t developed the requisite mathematical tools yet).</p> <p>We believe that all these containments are strict, for if it turns out that even one of the containments holds the other way as well (that is, both classes are one and the same), then the polynomial hierarchy ‘collapses’ to that level.</p> <p>Put more formally, it means that in the event that \(\Sigma_{i+1}^p = \Sigma_i^p\) holds for some particular \(i\), then it would mean that \(\Sigma_j^p = \Sigma_i^p\) for all \(j&gt;i\) (similarly for \(\Pi_i^p\)). In other words, the polynomial hierarchy collapses to level i. The other case where it would collapse is if \(\Pi_i^p = \Sigma_i^p\) for some \(i\).</p> <p>It is unnatural that the polynomial hierarchy would collapse, since it definitely seems like we are getting more ‘power’ by adding more quantifiers. This is also one of the reasons why many believe that \(\mathbf{P} \neq \mathbf{NP}\), and that \(\mathbf{NP} \neq \mathbf{coNP}\).</p> <h4 id="relationship-between-pspace-and-ph">Relationship between PSPACE and PH</h4> <p>One would realize after a moment’s thought that \(\mathbf{PH} \subseteq \mathbf{PSPACE}\), because you only need a polynomial amount of space to simulate the Turing Machine denoted by ‘\(M\)’ in the definitions, and a variable whose value you would toggle if a zero was outputted by one of the computations. Of course, as mentioned earlier, we don’t know whether this containment is strict or not.</p> <h2 id="oracles">Oracles</h2> <p>An Oracle Turing machine is a Turing machine with access to a certain oracle. Oracles are machines which solve the decision problem for some language \(O\) in just a single computational step, no matter how computationally hard \(O\) is (the oracle itself is denoted by \(O\) too). At any point during its computation, a TM can write down a string and ask the oracle, ‘does this string lie in O?’, and get back an answer in the very next step. It can query the oracle as many times as it wants to, on any random string of its choosing.</p> <p>By the definition, it is obvious that the oracle gives the TM some additional power, and also that the oracle is just a ‘convenience’ which allows us to black box the hardness of computing any particular language, and that it does not have any real world analogues.</p> <p>A few more definitions before we get to the next section (where I actually start discussing the paper (!)): a language is denoted by \(L^O\) if there is a TM with access to oracle \(O\) which decides \(L\). The complexity class \(\mathbf{P^O}\) is defined as the set of all languages which can be decided by a polynomial time TM with access to oracle \(O\). The analogues to other complexity classes are defined in a similar manner.</p> <p>As a reminder, the goal of the paper that I’m discussing here is to give some evidence for the belief that \(\mathbf{PH} \subset \mathbf{PSPACE}\), and showing that \(\mathbf{PH^O} \subset \mathbf{PSPACE^O}\), where \(O\) is some valid oracle, is one way of doing so, and this paper gives us such an oracle.</p> <h2 id="train-of-thought">Train of thought</h2> <p>We will first define a certain language which we’ll call \(L^A\) (the TM which decides this language has access to oracle A), and show that it lies in \(\mathbf{PSPACE^A}\). Then we assume that \(L^A \in \mathbf{PH^A}\), which means that there’s a level \(i\) in the polynomial hierarchy in which \(L^A\) is contained (\(L^A \in \Sigma_i^{p, A}\) for some \(i\)). Next, it is shown that given the previous assumption (\(L^A \in \mathbf{PH^A}\)), there must exist a \(O(polylog(n))\) circuit computing the parity function.</p> <p>But this is where it gets interesting! The second part of the paper (which I won’t be discussing) proves that a \(O(polylog(n))\) sized parity-computing circuit doesn’t exist, which means \(L^A\) is not in \(\Sigma_i^{p, A}\) for any \(i\), which means \(L^A \not\in \mathbf{PH^A}\). Thus, we will have proved that \(\mathbf{PH^A} \subset \mathbf{PSPACE^A}\).</p> <h2 id="first-step-defining-la">First Step: Defining \(L^A\)</h2> \[L^A = \{1^n | \text{Number of n-length strings in A is odd}\}\] <p>Assume \(A\) to be some language. The language \(L^A\) is defined relative to the strings present in \(A\). It only happens to contain at most one string of any given length, and if it does happen to contain a certain string of length \(n\), then it will be made up of all ones. What is the condition under which \(L^A\) will contain an \(n\)-length, all-ones string? Only when the language A has an odd number of \(n\)-length strings in it.</p> <p>Right away, by looking at the definition of \(L^A\), it seems obvious that it has some connection to the parity function. In fact, it has been defined such that it has this connection, which we’ll exploit later. What’s also obvious is that this language exists in \(\mathbf{PSPACE^A}\). Consider the following procedure: upon being given a \(n\)-length string, proceed further only if it is an all-ones string (reject if not). Then, simply query oracle A all possible \(2^n\) strings of length \(n\), and accept only if the number is odd. All this can easily be done by a TM having space polynomial in the size of input.</p> <h2 id="second-step-showing-existence-of-an-opolylogn-sized-parity-computing-circuit">Second Step: Showing existence of an \(O(polylog(n))\) sized parity-computing circuit</h2> <p>We’ll be showing this under the assumption that \(L^A\) is in \(\mathbf{PH^A}\). As stated above, this implies that \(L^A \in \Sigma_i^{p, A}\). This means that:</p> \[x \in L^A \iff \{ \exists y_1 \forall y_2 \ldots Q_iy_i s.t. M^A(y_1, y_2, \ldots , y_i, x) = 1\}\] <p>where \(M^A\) is a TM with the modification that is has access to oracle A. Next, we claim that:</p> \[M^A(y_1, y_2, ... y_i, x) = \exists r \forall s \overline{M^A}(y_1, y_2, ... y_i, r, s, x)\] <p>Where \(\overline{M^A}\) is similar to \(M^A\), but with two modifications: one, it can only make one oracle query (\(M^A\), of course, can make a polynomial number of queries to the oracle), and two, it has two additional inputs. Note the trade off here: \(\overline{M^A}\) is ‘equal’ in power to \(M^A\), even though the number of queries which can be asked has been restricted. This is because we have now ‘ascended’ two levels higher in the polynomial hierarchy. But why should the trade-off be true?</p> <p>To see this, consider the machine \(M^O\), whose computations are identical in nature to \(M^A\), except that we can swap in any oracle we want. Now fix an input \(x\), along with some valid combination of \(y_1, ... y_i\), and then think of the computation of a certain TM \(M^O\) in the following manner: Each time the TM enters into a state where it queries the oracle, it can end up in one of two states, depending upon the oracle (or, more precisely, its answer to that query). Therefore, if we decide that we want to ‘track’ all possible ‘computational paths’ down which the TM might go, we will notice that we get a binary tree. Each path will correspond to a different oracle. Furthermore, as long as the inputs are fixed, the computational path taken by the TM is wholly dependent on the responses of the oracle (think about why this is true).</p> <p>Now, suppose we fix an input \(x\) which is in \(L^A\), and also some combination of \(y_1, ..., y_i\), which automatically fixes a corresponding computation graph for \(M^O\). Recalling the definition of \(\Sigma_i^{p, A}\), we can see that \(M^A\) must output \(1\) no matter what \(y_1, ..., y_i\) we have chosen. Therefore, since we now know that there exists at least one oracle for which \(M^O\) outputs 1, we can be sure that there exists a branch in the graph whose query-response pairs at each ‘branching’ matches the response of oracle A, when it is queried that particular query. This branch is the exact branch taken by \(M^A\). Additionally, the state at the end of the path will be an accepting one.</p> <p>Now, it is easier to see how the two machines are equivalent. Take ‘\(r\)’ to be an accepting path, which is characterized by a sequence of query-response pairs, and allow ‘\(s\)’ to range over all possible queries. Therefore, given a particular \(r\) and \(s\), what \(\overline{M^A}\) will do is it will simulate \(M^A\), and each time it reaches a point where it has to query the oracle, it just uses the corresponding response from the sequence represented by ‘\(r\)’. In case the query matches the query represented by ‘\(s\)’, \(\overline{M^A}\) will actually query the oracle and indeed verify that the query-response pair is indeed right. If the response does not match the one given by the oracle, then the machine rejects (this is the only case where \(\overline{M^A}\) will reject, and if \(x\) does not belong to \(L^A\), there will be an incorrect query response pair in ‘\(r\)’, and a corresponding query ‘\(s\)’, so \(\overline{M^A}\) will reject when it comes to that ‘\(s\)’). Stepping back, it is easy to see that if \(x\) actually belongs to \(L^A\), then \(\overline{M^A}\) will, by iterating through all possible queries, chance upon each query specified in the sequence represented by ‘\(r\)’, and will verify the whole sequence correctly.</p> <p>A thing to note here is that \(\exists r \text{ } \forall s \overline{M^A}(y_1, y_2, ... y_i, r, s, x)\) is equivalent to \(\forall s \text{ } \exists r \overline{M^A}(y_1, y_2, ... y_i, r, s, x)\) since the variables ‘\(r\)’ and ‘\(s\)’ are unrelated to each other. Also note that this switching is not possible in every case. For example, the \(MIN-CNF\) problem described above, lies in \(\Pi_2^p\), but does not lie in \(\Sigma_2^p\) (at least, we don’t know that it does), because switching the variables is not possible in this case.</p> <p>We are in a position to go a step further and claim that \(L^A\) in fact lies in \(\Sigma_{i+1}^{p,A}\) (with regards to the \(\overline{M^A}\) machine, that is. It is trivially in \(\Sigma_{i+1}^{p,A}\) for the \(M^A\) machine). Depending upon whether \(Q_i\) is \(\forall\) or \(\exists\), we replace \(M^A(...)\) with the equivalent \(\forall s \text{ } \exists r \text{ } \overline{M^A}(...)\) or \(\exists r \text{ } \forall s \text{ } \overline{M^A}(...)\) version, and then the two consecutive universal (or existential) quantifiers can be merged into one.</p> <p>We are now almost ready to construct the parity-computing circuit, we just need a couple of things in order.</p> <p>First, think of the following NP statement:</p> \[x \in L \iff \exists y \text{ } M(x, y) = 1\] <p>This can be thought of as (I hope you will forgive me for the bad drawings that follow!):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/exists-480.webp 480w,/assets/img/exists-800.webp 800w,/assets/img/exists-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/exists.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where the \(\exists\) node acts like the OR gate, outputting \(1\) only when at least one of \(M(x, y_i)\) evaluates to \(1\). You can think of a similar construction for coNP:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/forall-480.webp 480w,/assets/img/forall-800.webp 800w,/assets/img/forall-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/forall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>A construction for \(\Sigma_2^p\) will look like:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/composite-480.webp 480w,/assets/img/composite-800.webp 800w,/assets/img/composite-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/composite.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Now, extending this concept a bit further, we can think of any \(\Sigma_i^p\) computation as a tree of nodes, with the levels alternating between \(\exists\) and \(\forall\). Each of the leaves would then correspond to a computation, with the inputs as some particular combination of \(y_1,...y_n\). It is quite easy to see that the construction outputs \(1\) if and only if \(\exists y_1... M^A(y_1, ..)\) evaluates to true.</p> <p>Now, consider the \(\Sigma_{i+1}^p\) computation where we use \(\overline{M^A}(y_1, y_2, ..., y_{i+1}, x)\). Each particular computation’s output, it turns out, can be associated with the response of the query it makes to the oracle during the course of its computation (Remember, \(\overline{M^A}\) only makes a single query).</p> <p>The query made to the oracle is one of \(\approx 2^n\) possible queries, which can be denoted by \(q_1, q_2, ..., q_{2^n}\), and their corresponding responses by \(r_1, r_2, ..., r_{2^n}\). Suppose during computing \(\overline{M^A}(y_1, y_2, ..., y_{i+1}, x)\), the machine queries the oracle for \(q_j\) and receives \(r_j\) as response. Since the output of \(\overline{M^A}(y_1, y_2, ..., y_{i+1}, x)\) and the quantity \(r_j\) are both boolean variables, either one of \(\overline{M^A}(y_1, y_2, ..., y_{i+1}, x) = r_j\) or \(\overline{M^A}(y_1, y_2, ..., y_{i+1}, x) = \overline{r_j}\) holds true.</p> <p>It follows that we can label the leaves of the computation tree with \(r_j\) (or \(\overline{r_j}\)) now (note that this does not mean that the construction only has \(\approx 2^n\) leaves. In fact, the number of leaves far exceeds that number). Now, by having an understanding of how we can construct a ‘\(\forall\exists\) circuit’, and knowing that the particular construction we created outputs \(1\) only when the number of \(n\)-length strings in A is odd, and by modifying the circuit by replacing every \(\forall\) node with an \(AND\) gate, every \(\exists\) node with an \(OR\) gate, we get a circuit which computes the parity of the \(2^n\) bits represented by the bits (\(r_j\) or \(\overline{r_j}\)) at the leaves of the circuit.</p> <p>Going the other way, we can look at it as follows: Given \(2^n\) input bits (if the number of bits in the input isn’t a power of \(2\), pad it with zeroes. The parity value won’t change!), label them as \(r_1, r_2, ..., r_{2^n}\). Now arrange all the \(2^n\) \(n\)-length strings in lexicographical order, and add the \(i\)th string to a set if \(r_i = 1\). This set is now the language \(A\). Therefore, \(L^A\) will contain \(1^n\) only if parity of the input bits is equal to 1, and everything else follows. It might be the most convoluted way ever to calculate parity, but it works.</p> <p>One other useful thing of note is that during the computation of \(M^A\), where \(M^A\) has to decide whether or not a given \(x\) is present in \(L^A\) or not, it will query the oracle of every possible \(n\)-length string, \(n\) being the length of \(x\). How do we know this? Because of how \(L^A\) is defined, the inclusion of string \(x\) in it depends upon the fact that it is completely made up of ones, and that an odd number of \(n\)-length strings are present in A. Therefore, since \(M^A\) does not have any ‘built-in’ knowledge of language A, and because of the fact that it can only know anything all about A is by querying the oracle, and because if the machine misses out on querying even a single \(n\)-length string it may give the wrong answer, it has no choice but to query every possible string. For those in the know, this is a consequence of the fact that the communication complexity of the parity function is \(n\).</p> <p>The only thing left to do is to derive the size of the circuit. Since every variable \(y_i\) is of length at most polynomial in \(n\), assume it is of length at most \(n^k\), for some \(k\). Then, it follows that every internal node must have \(2^{n^k}\) children, one for each possible value of \(y_i\). Using this, the size of the circuit comes out to be \(O(2^{n(log^k(2^n))})\). Since we are using a \(2^n\) sized string as input, if we replace \(2^n\) by \(n'\), we have an \(O(n'^{(log^k(n'))})\) sized circuit for computing parity.</p> <p>This proof not just proves the existence of a \(O(n^{log^k(n)})\) sized circuit for computing the parity of \(n\) bits, but gives details for constructing one as well. Of course, this is under the assumption that \(L^A\) lies in \(\mathbf{PH^A}\).</p> <p>The second section of the paper essentially proves the contrapositive of the above statement. It shows that a \(O(n^{log^k(n)})\) sized circuit for computing parity of \(n\) bits does not exist, thus \(L^A \not\in \mathbf{PH^A}\), proving the existence of an oracle relative to which \(\mathbf{PH} \subset \mathbf{PSPACE}\).</p> <p>There are a few additional implications about the second result in the paper, like multiplication and the majority function don’t lie in \(AC^0\) as well, but the most significant implication, by far, is the one discussed above.</p> <h2 id="conclusion">Conclusion</h2> <p>Oracle separations are an interesting way to glean evidence about things which we have no idea how tackle, given the available tools. A much more recent oracle separation was shown by <a href="https://engineering.princeton.edu/faculty/ran-raz">Ran Raz</a> and <a href="http://www.avishaytal.org/">Avishay Tal</a> in 2018, between <a href="https://en.wikipedia.org/wiki/BQP">\(BQP\)</a> and \(PH\) (it showed \(BQP^A \not\subset PH^A\), which is surprising enough in its own right).</p>]]></content><author><name></name></author><category term="complexity-theory"/><category term="exposition"/><summary type="html"><![CDATA[Introduction An important aspect of complexity theory is to try and find separations between different complexity classes. That is, to show that two complexity classes are not equal. One type of separation is a strict containment. It turns out that it is easier to prove containments (for example, give two complexity classes \(A\) and \(\), to show that \(A \subseteq B\), or the opposite, whichever one is true) but to show a strict containment (\(A \subset B\)) is more difficult (\(\mathbf{P}\) vs \(\mathbf{NP}\) being the best example of this). Obviously, a strict containment implies a separation between the two classes, but a containment doesn’t.]]></summary></entry><entry><title type="html">Off main thread HTML parsing in Servo</title><link href="https://nikhilshagri.github.io/blog/2017/gsoc-project/" rel="alternate" type="text/html" title="Off main thread HTML parsing in Servo"/><published>2017-08-23T18:41:57+00:00</published><updated>2017-08-23T18:41:57+00:00</updated><id>https://nikhilshagri.github.io/blog/2017/gsoc-project</id><content type="html" xml:base="https://nikhilshagri.github.io/blog/2017/gsoc-project/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Traditionally, browsers have been written as single threaded applications, and the html spec certainly seems to validate this statement. This makes it difficult to parallelize any task which a browser carries out, and we generally have to come up with innovative ways to do so.</p> <p>One such task is HTML parsing, and I have been working on parallelizing it this summer as part of my GSoC project. Since Servo is written in Rust, I’m assuming the reader has some basic knowledge about Rust. If not, check out this awesome <a href="https://doc.rust-lang.org/book/second-edition/">Rust book</a>. Done? Let’s dive straight into the details:</p> <h3 id="html-parser">HTML Parser</h3> <p>Servo’s HTML (and XML) parsing code live in <a href="https://github.com/servo/html5ever">html5ever</a>. Since this project concerns HTML parsing, I will only be talking about that. The first component we need to know about is the <code class="language-plaintext highlighter-rouge">Tokenizer</code>. This component is responsible for taking in raw input from a buffer and creating tokens, eventually sending them to its Sink, which we will call <code class="language-plaintext highlighter-rouge">TokenSink</code>. This could be any type which implements the <code class="language-plaintext highlighter-rouge">TokenSink</code> trait.</p> <p>html5ever has a type called <code class="language-plaintext highlighter-rouge">TreeBuilder</code>, which implements this trait. The TreeBuilder’s job is to create tree operations based on the tokens it receives. TreeBuilder contains its own Sink, called <a href="https://doc.servo.org/markup5ever/interface/tree_builder/trait.TreeSink.html">TreeSink</a>, which details the methods corresponding to these tree ops. The TreeBuilder calls these <code class="language-plaintext highlighter-rouge">TreeSink</code> methods under appropriate conditions, and these ‘action methods’ are responsible for constructing the DOM tree.</p> <p>With me so far? Good. The key to parallelizing HTML parsing is realizing that the task of creating tree ops is independent from the task of <em>actually</em> executing them to construct the DOM tree. Therefore, tokenization and tree op creation can happen on a separate thread, while the tree construction can be done on the main thread itself.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/parsing_diagram-480.webp 480w,/assets/img/parsing_diagram-800.webp 800w,/assets/img/parsing_diagram-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/parsing_diagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Information flow in both models: a comparision. </div> <h3 id="the-process">The Process</h3> <p>The first step I took was to decouple tree op creation from tree construction. Previously, tree ops were executed as soon as they were created. This involved the creation of a new <a href="https://github.com/servo/servo/blob/master/components/script/dom/servoparser/async_html.rs#L512">TreeSink</a>, which instead of executing them directly, created a <a href="https://github.com/servo/servo/blob/master/components/script/dom/servoparser/async_html.rs#L59-L105">representation</a> of a tree op, containing all relevant data. For the time being, I sent the tree op to a <code class="language-plaintext highlighter-rouge">process_op</code> function as soon as it was created, whereupon it was executed.</p> <p>Now that these two processes were independent, my next task consisted of creating a new thread, where the Tokenizer+TreeBuilder pair would live, to generate these tree ops. Now, when a tree op was created, it would be sent to the main thread, and control would return back to the TreeBuilder. The TreeBuilder does not have to wait for the execution of the tree op anymore, thus speeding up the entire process.</p> <p>So far so good. The final task in this project was to implement speculative parsing, by building on top of these recent changes.</p> <h3 id="speculative-parsing">Speculative Parsing</h3> <p>The HTML spec dictates that at any point during parsing, if we encounter a script tag, then the script must be executed immediately (if it is an inline script), or must be fetched and then executed (note that this rule does not apply to <code class="language-plaintext highlighter-rouge">async</code> or <code class="language-plaintext highlighter-rouge">defer</code> scripts). Why, you might ask, must this be done so? Why can’t we mark these scripts and execute them all at the end, after the parsing is done? This is because of an old, ill-thought out <code class="language-plaintext highlighter-rouge">Document</code> API function called <code class="language-plaintext highlighter-rouge">document.write()</code>. This function is a pain point for many developers who work on browsers, as it is a real headache implementing it well enough, while working around the many idiosyncrasies which surround it. I won’t dive into the details here, as they are not relevant. All we need to know is what <code class="language-plaintext highlighter-rouge">document.write()</code> does: it takes a string argument, which is generally markup, and inserts this string as part of the document’s HTML content. It is suffice to say that using this function might break your page, and should not be used.</p> <p>Returning to the parsing task, we can’t commit any DOM manipulations until the script finishes executing, because <code class="language-plaintext highlighter-rouge">document.write()</code> could make them redundant. What speculative parsing aims to do is to continue parsing the content after the script tag in the parser thread, while the script is being executed in the main thread. Note that we are only speculatively creating tree ops here, not the actual tree construction. After the script finishes executing, we analyze the actions of the <code class="language-plaintext highlighter-rouge">document.write()</code> calls (if any) to determine whether to use the tree ops, or to throw them away.</p> <h3 id="roadblock">Roadblock!</h3> <p>Remember when I said the process of creating tree ops is independent from tree construction? Well, I lied a little. Until a week ago, we need access to some DOM nodes for the creation of a couple of tree actions (one method needed to know if a node had a parent, and the other needed to know whether two nodes existed in the same tree). When I moved the task of creating tree ops to a separate thread, I could no longer access the DOM tree, which lived on the main thread. So I used a <code class="language-plaintext highlighter-rouge">Sender</code> on the TreeSink to create and send <a href="https://github.com/servo/servo/pull/17565/files#diff-10b46cb1e26142d2058e291de25bd4c7R133">queries</a> to the main thread, which would access the DOM and send the results back. Then only would the TreeSink method return, with the data it received from the main thread. Additionally, this meant that these couple of methods were synchronous in nature. No biggie.</p> <p>I realized the problem when I sat down to think about how I would implement speculative parsing. Since the main thread is busy executing scripts, it won’t be listening to the queries these synchronous methods will be sending, and therefore the task of creating tree ops cannot progress further!</p> <p>This turned out to be a bigger problem than I’d imagined, and I also had to sift through the equivalent Gecko code to understand how this situation was handled. I eventually came up with a good solution, but I won’t bore you with the details. If you want to know more, here’s a <a href="https://gist.github.com/nikhilshagri/09fb8a6dd1db58852d2085ac59ca0f9b">gist</a> explaining the solution.</p> <p>With these changes landed in html5ever, I can finally implement speculative parsing. Unfortunately, there’s not much time to implement it as a part of the GSoC project, so I will be landing this feature in Servo some time later. I hope to publish another blog post describing it thoroughly, along with details on the performance improvements this feature would bring.</p> <h4 id="links-to-important-prs">Links to important PRs:</h4> <p>Added Async HTML Tokenizer: <a href="https://github.com/servo/servo/pull/17037">https://github.com/servo/servo/pull/17037</a></p> <p>Run the async HTML Tokenizer on a new thread: <a href="https://github.com/servo/servo/pull/17914">https://github.com/servo/servo/pull/17914</a></p> <p>TreeBuilder no longer relies on <code class="language-plaintext highlighter-rouge">same_tree</code> and <code class="language-plaintext highlighter-rouge">has_parent_node</code>: <a href="https://github.com/servo/html5ever/pull/300">https://github.com/servo/html5ever/pull/300</a></p> <p>End TreeBuilder’s reliance on DOM: <a href="https://github.com/servo/servo/pull/18056">https://github.com/servo/servo/pull/18056</a></p> <h2 id="conclusion">Conclusion</h2> <p>This was a really fun project; I got to solve lots of cool problems, and also learnt a lot more about how a modern, spec-compliant rendering engine works.</p> <p>I would like to thank my mentor <a href="https://twitter.com/nokusu">Anthony Ramine</a>, who was absolutely amazing to work with, and <a href="https://twitter.com/lastontheboat">Josh Matthews</a>, who helped me a lot when I was still a rookie looking to contribute to the project.</p>]]></content><author><name></name></author><category term="programming"/><category term="gsoc"/><summary type="html"><![CDATA[Introduction Traditionally, browsers have been written as single threaded applications, and the html spec certainly seems to validate this statement. This makes it difficult to parallelize any task which a browser carries out, and we generally have to come up with innovative ways to do so.]]></summary></entry></feed>